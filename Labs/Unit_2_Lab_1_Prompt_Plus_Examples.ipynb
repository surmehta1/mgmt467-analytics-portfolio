{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surmehta1/mgmt467-analytics-portfolio/blob/main/Unit_2_Lab_1_Prompt_Plus_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoU9mpfTarDi"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "HoU9mpfTarDi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1FBm6lwarDm"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "W1FBm6lwarDm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qClqx7VMarDm"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "qClqx7VMarDm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHv3IyltarDm"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "wHv3IyltarDm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBxZax9KarDn"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "jBxZax9KarDn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXY_RZk6arDn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "# # from google.colab import auth\n",
        "# # auth.authenticate_user()\n",
        "# #\n",
        "# # import os\n",
        "# # PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "# # REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "# # os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "# # print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "# #\n",
        "# # # Set active project for gcloud/BigQuery CLI\n",
        "# # !gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "# # !gcloud config get-value project\n",
        "# # # Done: Auth + Project/Region set"
      ],
      "id": "NXY_RZk6arDn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b49c50f",
        "outputId": "57839d05-70a6-478d-bb4c-3d01a1458dda"
      },
      "source": [
        "# Authenticate to Google Cloud in Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "# Prompt for PROJECT_ID and set REGION\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "1b49c50f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt467-471119\n",
            "Project: mgmt467-471119 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt467-471119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys8LPzLBarDo"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "Ys8LPzLBarDo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGHgLXrarDo"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "AUGHgLXrarDo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk0sOb9XarDp"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "hk0sOb9XarDp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGjLL6H6arDp"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "RGjLL6H6arDp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIcCA8G9arDp"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "# # from google.colab import files\n",
        "# # print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "# # uploaded = files.upload()\n",
        "# #\n",
        "# # import os\n",
        "# # os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# # with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "# #     f.write(uploaded[list(uploaded.keys())[0]])\n",
        "# # os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "# #\n",
        "# # !kaggle --version"
      ],
      "id": "CIcCA8G9arDp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "8ebef4fd",
        "outputId": "f114ecba-ace6-4953-f396-f12ec2006c51"
      },
      "source": [
        "# Set up Kaggle API\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "id": "8ebef4fd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-165bbea0-5332-43bd-b09f-608b379c2ab5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-165bbea0-5332-43bd-b09f-608b379c2ab5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ea4d817",
        "outputId": "df8d3d6c-c735-4cd9-fe68-171bf9771494"
      },
      "source": [
        "# Verification: print active project and region\n",
        "!gcloud config get-value project\n",
        "print(\"REGION:\", os.environ.get(\"REGION\"))"
      ],
      "id": "6ea4d817",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mgmt467-471119\n",
            "REGION: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAlogvJvarDp"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "hAlogvJvarDp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGpS1AU9arDq"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "XGpS1AU9arDq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jie-_wJrarDq"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "jie-_wJrarDq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ODpR0czarDq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "3ODpR0czarDq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg05lAwWarDq"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Download & unzip (commented)\n",
        "# # !mkdir -p /content/data/raw\n",
        "# # !kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "# # !unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# # # List CSV inventory\n",
        "# # !ls -lh /content/data/raw/*.csv"
      ],
      "id": "qg05lAwWarDq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a29f4765",
        "outputId": "62eac63a-68ff-4ac1-dbf5-4e59f114d9c8"
      },
      "source": [
        "# Download and unzip dataset\n",
        "!mkdir -p /content/data/raw\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# List CSV inventory\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "id": "a29f4765",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "netflix-2025user-behavior-dataset-210k-records.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuRtZB91arDq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "zuRtZB91arDq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lEjCU5larDr"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "0lEjCU5larDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2gLHBwdarDr"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "v2gLHBwdarDr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1363ce21",
        "outputId": "0ad0eb5c-7430-4d4f-f2b1-12b13b9c377a"
      },
      "source": [
        "# Create BigQuery dataset (idempotent)\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "1363ce21",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'mgmt467-471119:netflix' already exists.\n",
            "Dataset may already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8d91b94",
        "outputId": "13b2c9e1-6c5c-430a-adf6-b377b9fa7dc5"
      },
      "source": [
        "# Load tables from GCS into BigQuery\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "\n",
        "import os\n",
        "# Ensure BUCKET_NAME is set before proceeding\n",
        "if \"BUCKET_NAME\" not in os.environ:\n",
        "  print(\"Error: BUCKET_NAME environment variable is not set.\")\n",
        "  print(\"Please run the cell to create and upload data to the GCS bucket first.\")\n",
        "else:\n",
        "  for tbl, fname in tables.items():\n",
        "    src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "    print(\"Loading\", tbl, \"from\", src)\n",
        "    !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "  # Row counts\n",
        "  for tbl in tables.keys():\n",
        "    !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\""
      ],
      "id": "d8d91b94",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-471119-netflix-60e311fb/netflix/users.csv\n",
            "Waiting on bqjob_rece09e24026369c_0000019a178b0c19_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-471119-netflix-60e311fb/netflix/movies.csv\n",
            "Waiting on bqjob_raebe44d7a21e8b3_0000019a178b236d_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-471119-netflix-60e311fb/netflix/watch_history.csv\n",
            "Waiting on bqjob_r5bf1abe80b1a0f5c_0000019a178b3a2a_1 ... (3s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-471119-netflix-60e311fb/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r10a121573a1d6727_0000019a178b5a14_1 ... (1s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-471119-netflix-60e311fb/netflix/search_logs.csv\n",
            "Waiting on bqjob_r50ce06c29bfc3e54_0000019a178b731b_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-471119-netflix-60e311fb/netflix/reviews.csv\n",
            "Waiting on bqjob_r52fe8837d31cb4d1_0000019a178b898a_1 ... (1s) Current status: DONE   \n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r15802ef7e4fb2256_0000019a178ba214_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r5dee3293ae7f879e_0000019a178baed9_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r7761fc419ab9969c_0000019a178bbaff_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r28acf718cfaacbdf_0000019a178bc6b0_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r784c499e6a0e31a_0000019a178bd5a7_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r4828b50aac537673_0000019a178be15e_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd273f6",
        "outputId": "e4ce0c80-2870-4c3d-a875-8cfa792b8eb6"
      },
      "source": [
        "# Create a unique bucket in the specified region and upload data\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "bucket_name = f\"{PROJECT_ID}-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "print(f\"Created bucket: {bucket_name}\")\n",
        "print(\"Dataset files uploaded to GCS. Staging data in GCS provides a centralized, versionable source for loading into BigQuery and other services.\")\n",
        "\n",
        "# Verify contents (optional)\n",
        "# !gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "3fd273f6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mgmt467-471119-netflix-a67fb9b7/...\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-471119-netflix-a67fb9b7/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-471119-netflix-a67fb9b7/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-471119-netflix-a67fb9b7/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-471119-netflix-a67fb9b7/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-471119-netflix-a67fb9b7/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-471119-netflix-a67fb9b7/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 60.3MiB/s\n",
            "Created bucket: mgmt467-471119-netflix-a67fb9b7\n",
            "Dataset files uploaded to GCS. Staging data in GCS provides a centralized, versionable source for loading into BigQuery and other services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "176ac0dd",
        "outputId": "05e07678-1b08-4e3e-c642-e0fde48aef1b"
      },
      "source": [
        "# Verification: List contents of the GCS bucket\n",
        "import os\n",
        "bucket_name = os.environ.get(\"BUCKET_NAME\")\n",
        "if bucket_name:\n",
        "  !gcloud storage ls gs://$BUCKET_NAME/netflix/ --recursive --readable-sizes\n",
        "else:\n",
        "  print(\"BUCKET_NAME environment variable not set.\")"
      ],
      "id": "176ac0dd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/:\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/movies.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/reviews.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/search_logs.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/users.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qmcODTAarDr"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "_qmcODTAarDr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktBV4vararDr"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — GCS staging (commented)\n",
        "# # import uuid, os\n",
        "# # bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "# # os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "# # !gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "# # !gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "# # print(\"Bucket:\", bucket_name)\n",
        "# # # Verify contents\n",
        "# # !gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "ktBV4vararDr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6735465",
        "outputId": "dd697b3d-354a-4546-8482-efcab947ef52"
      },
      "source": [
        "# Create BigQuery dataset (idempotent)\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "c6735465",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'mgmt467-471119:netflix' already exists.\n",
            "Dataset may already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1382c904",
        "outputId": "45a72037-74e4-4996-afdc-a07acc2f723e"
      },
      "source": [
        "# Load tables from GCS into BigQuery\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "\n",
        "import os\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  # Fixed bq load command syntax\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts\n",
        "for tbl in tables.keys():\n",
        "  # Fixed bq query command syntax\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\""
      ],
      "id": "1382c904",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-471119-netflix-a67fb9b7/netflix/users.csv\n",
            "Waiting on bqjob_r2bc07cacddee0307_0000019a178c19c1_1 ... (3s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-471119-netflix-a67fb9b7/netflix/movies.csv\n",
            "Waiting on bqjob_r4ec27650eb49a0ad_0000019a178c3a1f_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-471119-netflix-a67fb9b7/netflix/watch_history.csv\n",
            "Waiting on bqjob_r40a2f0cebcde4d3d_0000019a178c5058_1 ... (3s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-471119-netflix-a67fb9b7/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r2f4b14c367c42c38_0000019a178c6ea8_1 ... (2s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-471119-netflix-a67fb9b7/netflix/search_logs.csv\n",
            "Waiting on bqjob_r6a4cf061de9c7759_0000019a178c8ac7_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-471119-netflix-a67fb9b7/netflix/reviews.csv\n",
            "Waiting on bqjob_r5fbe36a7561e1688_0000019a178ca1c4_1 ... (1s) Current status: DONE   \n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r5cfc0b3b04ad9269_0000019a178cb878_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r7fc3dac204e4fcfe_0000019a178cc64b_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r1b7c4bfdffb24e3f_0000019a178cd25e_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r73db17435116f80c_0000019a178cdedf_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r5be45682485d1f54_0000019a178cec8b_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.{tbl}: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r31afefbb805766f7_0000019a178cf9dd_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23342a83",
        "outputId": "e7e9de85-f098-44d2-85c6-cf2b8c5fa566"
      },
      "source": [
        "# Verification: List contents of the GCS bucket\n",
        "import os\n",
        "bucket_name = os.environ.get(\"BUCKET_NAME\")\n",
        "if bucket_name:\n",
        "  !gcloud storage ls gs://$BUCKET_NAME/netflix/ --recursive --readable-sizes\n",
        "else:\n",
        "  print(\"BUCKET_NAME environment variable not set.\")"
      ],
      "id": "23342a83",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/:\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/movies.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/reviews.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/search_logs.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/users.csv\n",
            "gs://mgmt467-471119-netflix-a67fb9b7/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f767e98f",
        "outputId": "95cb8c1d-ac93-4e67-f8da-0d5439b709fe"
      },
      "source": [
        "# Create a unique bucket in the specified region and upload data\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "bucket_name = f\"{PROJECT_ID}-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "print(f\"Created bucket: {bucket_name}\")\n",
        "print(\"Dataset files uploaded to GCS. Staging data in GCS provides a centralized, versionable source for loading into BigQuery and other services.\")\n",
        "\n",
        "# Verify contents (optional)\n",
        "# !gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "f767e98f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mgmt467-471119-netflix-ce0628a4/...\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-471119-netflix-ce0628a4/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-471119-netflix-ce0628a4/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-471119-netflix-ce0628a4/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-471119-netflix-ce0628a4/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-471119-netflix-ce0628a4/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-471119-netflix-ce0628a4/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 59.7MiB/s\n",
            "Created bucket: mgmt467-471119-netflix-ce0628a4\n",
            "Dataset files uploaded to GCS. Staging data in GCS provides a centralized, versionable source for loading into BigQuery and other services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1HCT6X_arDr"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "a1HCT6X_arDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O-vcGnGarDr"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "8O-vcGnGarDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sorjq6WqarDr"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "sorjq6WqarDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-PW8wmarDs"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "44-PW8wmarDs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgq16HSParDs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "# # DATASET=\"netflix\"\n",
        "# # # Attempt to create; ignore if exists\n",
        "# # !bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "Pgq16HSParDs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0eb418c",
        "outputId": "8ea99fa2-ebfa-4cc2-b43a-0e31613dffcd"
      },
      "source": [
        "# Verification: Get row counts for all tables\n",
        "!bq query --nouse_legacy_sql \"SELECT table_name, row_count FROM `{GOOGLE_CLOUD_PROJECT}.netflix.__TABLES__`\""
      ],
      "id": "c0eb418c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: {GOOGLE_CLOUD_PROJECT}.netflix.__TABLES__: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-471119:bqjob_r5edad692c7c37273_0000019a178d2510_1': Syntax error:\n",
            "Unexpected end of script at [1:34]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT-ay4_5arDs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Load tables (commented)\n",
        "# # tables = {\n",
        "# #   \"users\": \"users.csv\",\n",
        "# #   \"movies\": \"movies.csv\",\n",
        "# #   \"watch_history\": \"watch_history.csv\",\n",
        "# #   \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "# #   \"search_logs\": \"search_logs.csv\",\n",
        "# #   \"reviews\": \"reviews.csv\",\n",
        "# # }\n",
        "# # import os\n",
        "# # for tbl, fname in tables.items():\n",
        "# #   src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "# #   print(\"Loading\", tbl, \"from\", src)\n",
        "# #   !bq load --skip_leading_rows=1 --autodetect --source_format=CSV $DATASET.$tbl $src\n",
        "# #\n",
        "# # # Row counts\n",
        "# # for tbl in tables.keys():\n",
        "# #   !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `${GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\".format(tbl=tbl)"
      ],
      "id": "TT-ay4_5arDs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3a67f01",
        "outputId": "537346dd-4083-4cae-c83c-971360a69b38"
      },
      "source": [
        "%%bigquery\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(region IS NULL) miss_region,\n",
        "         COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "         COUNTIF(age_band IS NULL) miss_age\n",
        "  FROM `@GOOGLE_CLOUD_PROJECT`.netflix.users\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(SAFE_DIVIDE(100*miss_region, n),2) AS pct_missing_region,\n",
        "       ROUND(SAFE_DIVIDE(100*miss_plan, n),2)   AS pct_missing_plan_tier,\n",
        "       ROUND(SAFE_DIVIDE(100*miss_age, n),2)    AS pct_missing_age_band\n",
        "FROM base;"
      ],
      "id": "b3a67f01",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: bbe75be0-7cb8-4498-8567-56956e4f2c6c\n",
            "\rQuery executing: 0.21s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '@GOOGLE_CLOUD_PROJECT'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: @GOOGLE_CLOUD_PROJECT.netflix.users, message: Invalid project ID '@GOOGLE_CLOUD_PROJECT'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: bbe75be0-7cb8-4498-8567-56956e4f2c6c\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e8e9834",
        "outputId": "c6a958c1-3776-455b-bfc0-54270e191203"
      },
      "source": [
        "%%bigquery\n",
        "-- % plan_tier missing by region (MAR analysis)\n",
        "SELECT region,\n",
        "       COUNT(*) AS n,\n",
        "       ROUND(SAFE_DIVIDE(100*COUNTIF(plan_tier IS NULL), COUNT(*)),2) AS pct_missing_plan_tier\n",
        "FROM `@GOOGLE_CLOUD_PROJECT`.netflix.users\n",
        "GROUP BY region\n",
        "ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "8e8e9834",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 16fc0a10-c8f4-49e6-989f-ed214549e49d\n",
            "\rQuery executing: 0.20s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '@GOOGLE_CLOUD_PROJECT'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: @GOOGLE_CLOUD_PROJECT.netflix.users, message: Invalid project ID '@GOOGLE_CLOUD_PROJECT'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 16fc0a10-c8f4-49e6-989f-ed214549e49d\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt5fLsAIarDs"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "vt5fLsAIarDs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PPedmMQarDs"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "1PPedmMQarDs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tLnWq4VarDt"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "-tLnWq4VarDt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydu58aHcarDt"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "ydu58aHcarDt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtdcQqQTarDt"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "WtdcQqQTarDt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ziJt6k4arDt"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "# # -- Users: % missing per column\n",
        "# # WITH base AS (\n",
        "# #   SELECT COUNT(*) n,\n",
        "# #          COUNTIF(region IS NULL) miss_region,\n",
        "# #          COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "# #          COUNTIF(age_band IS NULL) miss_age\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # )\n",
        "# # SELECT n,\n",
        "# #        ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "# #        ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "# #        ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "# # FROM base;"
      ],
      "id": "7ziJt6k4arDt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74a65237",
        "outputId": "e36cca75-f78d-4829-ab2f-6d0dea74c54a"
      },
      "source": [
        "%%bigquery\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(region IS NULL) miss_region,\n",
        "         COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "         COUNTIF(age_band IS NULL) miss_age\n",
        "  FROM `your_project_id.netflix.users` -- REPLACE 'your_project_id' with your actual GCP Project ID\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(SAFE_DIVIDE(100*miss_region, n),2) AS pct_missing_region,\n",
        "       ROUND(SAFE_DIVIDE(100*miss_plan, n),2)   AS pct_missing_plan_tier,\n",
        "       ROUND(SAFE_DIVIDE(100*miss_age, n),2)    AS pct_missing_age_band\n",
        "FROM base;"
      ],
      "id": "74a65237",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 1394776a-2e7d-40b5-8a2b-659f5f76c8ff\n",
            "\rQuery executing: 0.19s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID 'your_project_id'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: your_project_id.netflix.users, message: Invalid project ID 'your_project_id'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 1394776a-2e7d-40b5-8a2b-659f5f76c8ff\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90307c7e",
        "outputId": "b7be996b-ab87-4ae7-9bc7-bb5793d8c911"
      },
      "source": [
        "%%bigquery\n",
        "-- % plan_tier missing by region (MAR analysis)\n",
        "SELECT region,\n",
        "       COUNT(*) AS n,\n",
        "       ROUND(SAFE_DIVIDE(100*COUNTIF(plan_tier IS NULL), COUNT(*)),2) AS pct_missing_plan_tier\n",
        "FROM `your_project_id.netflix.users` -- REPLACE 'your_project_id' with your actual GCP Project ID\n",
        "GROUP BY region\n",
        "ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "90307c7e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 61f164f9-0829-41c5-9476-6b25436feccf\n",
            "\rQuery executing: 0.20s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID 'your_project_id'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: your_project_id.netflix.users, message: Invalid project ID 'your_project_id'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 61f164f9-0829-41c5-9476-6b25436feccf\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUGgV9gIarDt"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "# # SELECT region,\n",
        "# #        COUNT(*) AS n,\n",
        "# #        ROUND(100*COUNTIF(plan_tier IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # GROUP BY region\n",
        "# # ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "GUGgV9gIarDt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYfdfmCearDu"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "YYfdfmCearDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTWHWfhHarDu"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "id": "cTWHWfhHarDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex_sEyqOarDu"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "ex_sEyqOarDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVMbm3JbarDu"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "BVMbm3JbarDu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s_zoQc6arDv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "# # SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "# # GROUP BY user_id, movie_id, event_ts, device_type\n",
        "# # HAVING dup_count > 1\n",
        "# # ORDER BY dup_count DESC\n",
        "# # LIMIT 20;"
      ],
      "id": "-s_zoQc6arDv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d28a767",
        "outputId": "029fd071-d201-43a6-a98d-6a077a0ceb8d"
      },
      "source": [
        "%%bigquery\n",
        "-- Report duplicate groups on (user_id, movie_id, event_ts, device_type) with counts (top 20)\n",
        "SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, event_ts, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;"
      ],
      "id": "6d28a767",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 1bd05fe4-be0e-48b6-949c-e6cc0257e319\n",
            "\rQuery executing: 0.15s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.watch_history, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 1bd05fe4-be0e-48b6-949c-e6cc0257e319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f9684a",
        "outputId": "080fd087-9318-4952-fb60-dbd0f6f06961"
      },
      "source": [
        "%%bigquery\n",
        "-- Create table watch_history_dedup that keeps one row per group (prefer higher progress_ratio, then minutes_watched)\n",
        "CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "           ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "         ) AS rk\n",
        "  FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;"
      ],
      "id": "88f9684a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: be6512c0-f292-4bc9-9e38-5d1c4b63b7dd\n",
            "\rQuery executing: 0.22s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: be6512c0-f292-4bc9-9e38-5d1c4b63b7dd\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks031m03arDv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "# # SELECT * EXCEPT(rk) FROM (\n",
        "# #   SELECT h.*,\n",
        "# #          ROW_NUMBER() OVER (\n",
        "# #            PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "# #            ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "# #          ) AS rk\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        "# # )\n",
        "# # WHERE rk = 1;"
      ],
      "id": "ks031m03arDv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UokozHfuarDw"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "UokozHfuarDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a19_oRlarDw"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?"
      ],
      "id": "9a19_oRlarDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-R_VFG2arDw"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "H-R_VFG2arDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPYC-w3rarDw"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "kPYC-w3rarDw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rePt8vl8arDw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "# # WITH dist AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(1)] AS q1,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(3)] AS q3\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # bounds AS (\n",
        "# #   SELECT q1, q3, (q3-q1) AS iqr,\n",
        "# #          q1 - 1.5*(q3-q1) AS lo,\n",
        "# #          q3 + 1.5*(q3-q1) AS hi\n",
        "# #   FROM dist\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi) AS outliers,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h\n",
        "# # CROSS JOIN bounds b;"
      ],
      "id": "rePt8vl8arDw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1f9c58",
        "outputId": "fa2d155a-455b-4118-f78c-198399c193f6"
      },
      "source": [
        "%%bigquery\n",
        "-- Compute IQR bounds for minutes_watched on watch_history_dedup and report % outliers.\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(minutes_watched, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(minutes_watched, 4)[OFFSET(3)] AS q3\n",
        "  FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;"
      ],
      "id": "2c1f9c58",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 27d82ea8-8234-4bbd-a9b4-4da6463cb9d8\n",
            "\rQuery executing: 0.21s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 27d82ea8-8234-4bbd-a9b4-4da6463cb9d8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a14d9bb",
        "outputId": "d8b6448f-51f2-4371-f7d4-55737d0fdaf7"
      },
      "source": [
        "%%bigquery\n",
        "-- Create watch_history_robust with minutes_watched_capped at P01/P99; return quantile summaries before/after.\n",
        "CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(minutes_watched, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(minutes_watched, 100)[OFFSET(98)] AS p99\n",
        "  FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.minutes_watched)) AS minutes_watched_capped\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(minutes_watched, 5) AS q\n",
        "  FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(minutes_watched_capped, 5) AS q\n",
        "  FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "id": "4a14d9bb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 7d73f38c-8580-4ff1-a86d-f01709762aa9\n",
            "\rQuery executing: 0.21s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/mgmt467-471119/queries/7d73f38c-8580-4ff1-a86d-f01709762aa9?maxResults=0&location=US&prettyPrint=false: Invalid value: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash. at [2:1]\n",
            "\n",
            "Location: US\n",
            "Job ID: 7d73f38c-8580-4ff1-a86d-f01709762aa9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4wPmNgKarDw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust` AS\n",
        "# # WITH q AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(1)]  AS p01,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(98)] AS p99\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   h.*,\n",
        "# #   GREATEST(q.p01, LEAST(q.p99, h.minutes_watched)) AS minutes_watched_capped\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h, q;\n",
        "# #\n",
        "# # -- Quantiles before vs after\n",
        "# # WITH before AS (\n",
        "# #   SELECT 'before' AS which, APPROX_QUANTILES(minutes_watched, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # after AS (\n",
        "# #   SELECT 'after' AS which, APPROX_QUANTILES(minutes_watched_capped, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        "# # )\n",
        "# # SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "id": "f4wPmNgKarDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw2E3YF6arDw"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "tw2E3YF6arDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3L_zxUNarDx"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "id": "y3L_zxUNarDx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp44FosDarDx"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "dp44FosDarDx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLC2hpWOarDx"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "hLC2hpWOarDx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECGXR5NqarDx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(minutes_watched > 8*60) AS sessions_over_8h,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(minutes_watched > 8*60)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`;"
      ],
      "id": "ECGXR5NqarDx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8d4556e",
        "outputId": "49d3d8e2-7d38-4c15-f8ee-51157831dfc6"
      },
      "source": [
        "%%bigquery\n",
        "-- Summarize flag_binge for sessions > 8 hours\n",
        "SELECT\n",
        "  COUNTIF(minutes_watched > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(SAFE_DIVIDE(100*COUNTIF(minutes_watched > 8*60), COUNT(*)),2) AS pct\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`;"
      ],
      "id": "d8d4556e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: ceff0dae-ee5e-415a-b7e7-e53b5c43cae3\n",
            "\rQuery executing: 0.22s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: ceff0dae-ee5e-415a-b7e7-e53b5c43cae3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42142310",
        "outputId": "da92e971-65f5-4bcd-8449-6a234563a4d2"
      },
      "source": [
        "%%bigquery\n",
        "-- Summarize flag_age_extreme if age can be parsed from age_band (<10 or >100)\n",
        "SELECT\n",
        "  COUNTIF(SAFE_CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "          SAFE_CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(SAFE_DIVIDE(100*COUNTIF(SAFE_CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "                                SAFE_CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100), COUNT(*)),2) AS pct\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`;"
      ],
      "id": "42142310",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 05a00d22-19f3-4a87-a1ef-ff423365ce89\n",
            "\rQuery executing: 0.18s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.users, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 05a00d22-19f3-4a87-a1ef-ff423365ce89\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b6f9ee3",
        "outputId": "d2c8d13e-a03c-4860-b1e4-cad771ab1f89"
      },
      "source": [
        "%%bigquery\n",
        "-- Summarize flag_duration_anomaly where duration_min < 15 or > 480\n",
        "SELECT\n",
        "  COUNTIF(duration_min < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_min > 480) AS titles_over_8h, -- 480 minutes = 8 hours\n",
        "  COUNT(*) AS total\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "2b6f9ee3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 46036206-6e42-413e-844b-87ea2199bbb7\n",
            "\rQuery executing: 0.17s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.movies, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 46036206-6e42-413e-844b-87ea2199bbb7\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9FIsOXiarDx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #           CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100) AS extreme_age_rows,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #                     CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`;"
      ],
      "id": "Q9FIsOXiarDx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2URoZidWarDx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(duration_min < 15) AS titles_under_15m,\n",
        "# #   COUNTIF(duration_min > 8*60) AS titles_over_8h,\n",
        "# #   COUNT(*) AS total\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "2URoZidWarDx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBEwRWOMarDx"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "iBEwRWOMarDx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46f09f80",
        "outputId": "d1812e32-0676-43cd-f38c-50924813092f"
      },
      "source": [
        "%%bigquery\n",
        "-- Compact summary of anomaly flags\n",
        "SELECT 'flag_binge' AS flag_name,\n",
        "       ROUND(SAFE_DIVIDE(100*COUNTIF(minutes_watched > 8*60), COUNT(*)),2) AS pct_of_rows\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        "UNION ALL\n",
        "SELECT 'flag_age_extreme' AS flag_name,\n",
        "       ROUND(SAFE_DIVIDE(100*COUNTIF(SAFE_CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "                                     SAFE_CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100), COUNT(*)),2) AS pct_of_rows\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly_under_15m' AS flag_name,\n",
        "       ROUND(SAFE_DIVIDE(100*COUNTIF(duration_min < 15), COUNT(*)),2) AS pct_of_rows\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly_over_8h' AS flag_name,\n",
        "       ROUND(SAFE_DIVIDE(100*COUNTIF(duration_min > 480), COUNT(*)),2) AS pct_of_rows -- 480 minutes = 8 hours\n",
        "FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "46f09f80",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: 5c9cce76-7348-4ab1-8d0a-80139279eb71\n",
            "\rQuery executing: 0.18s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; reason: invalid, location: ${GOOGLE_CLOUD_PROJECT}.netflix.movies, message: Invalid project ID '${GOOGLE_CLOUD_PROJECT}'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\n",
            "\n",
            "Location: US\n",
            "Job ID: 5c9cce76-7348-4ab1-8d0a-80139279eb71\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eef1ko3earDx"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "id": "eef1ko3earDx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output from the previous cells (which are currently failing due to a project ID issue), once they are fixed and run, you can analyze the percentage of rows for each flag. The flag with the highest percentage of rows would be the most common.\n",
        "\n",
        "Regarding which flag to keep as a feature, you would consider which anomaly is most likely to influence user behavior or be relevant for your modeling task. For example, flag_binge might be useful for identifying highly engaged users or predicting churn, while flag_age_extreme might indicate data entry errors or a niche user segment. flag_duration_anomaly in movies could highlight potential data issues or unique content. The decision depends on the specific business or ML problem you are trying to solve."
      ],
      "metadata": {
        "id": "fWA6isHpg0Lm"
      },
      "id": "fWA6isHpg0Lm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DObdFw0arDx"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "1DObdFw0arDx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoOLmh3LarDx"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "GoOLmh3LarDx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b3ab12d"
      },
      "source": [
        "## Checklist for Saving and Submitting\n",
        "\n",
        "*   [ ] Save this notebook to the team Drive.\n",
        "*   [ ] Export a `.sql` file with your DQ queries and save to repo.\n",
        "*   [ ] Push notebook + SQL to the team GitHub with a descriptive commit.\n",
        "*   [ ] Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts."
      ],
      "id": "3b3ab12d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHNo95hZarDy"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "mHNo95hZarDy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
