{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surmehta1/mgmt467-analytics-portfolio/blob/main/unit3_2_opensky_bq_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8f61cc6"
      },
      "source": [
        "## Unit 3 - Lab 2: Opensky to Big Query Table"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Python packages and authentication"
      ],
      "metadata": {
        "id": "SukrH0tPeFZ2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8908b17f",
        "outputId": "e02f3ae8-35b9-4f85-afca-64be0a6f1f4c"
      },
      "source": [
        "R\"\"\"\"\"\n",
        "This cell installs required Python packages and authenticates the user to Google Cloud.\n",
        "\n",
        "Packages installed:\n",
        "- google-cloud-storage: For interacting with Google Cloud Storage.\n",
        "- google-cloud-bigquery: For interacting with Google Cloud BigQuery.\n",
        "- requests: A popular HTTP library.\n",
        "\n",
        "Authentication:\n",
        "The cell authenticates the user to Google Cloud, enabling access to Google Cloud Platform (GCP) services.\n",
        "R\"\"\"\"\"\n",
        "!pip install google-cloud-storage google-cloud-bigquery requests\n",
        "\n",
        "from google.colab import auth\n",
        "print(\"Authenticating to Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authentication successful.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.28.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.8.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Authenticating to Google Cloud...\n",
            "Authentication successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "178da864"
      },
      "source": [
        "## Cell 2: Configure  project-specific variables and set the `gcloud` project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ab0f07",
        "outputId": "de7d292e-e5b0-43c6-f8ee-f795a842f576"
      },
      "source": [
        "R\"\"\"\n",
        "This cell configures essential project-specific variables for Google Cloud operations.\n",
        "\n",
        "It defines:\n",
        "- `PROJECT_ID`: The Google Cloud project ID.\n",
        "- `GCP_REGION`: The Google Cloud region for services.\n",
        "- `GCS_BUCKET_NAME`: The name of the Google Cloud Storage bucket.\n",
        "- `GCS_FOLDER_PATH`: The folder path within the GCS bucket for data storage.\n",
        "- `BQ_DATASET`: The BigQuery dataset name.\n",
        "- `BQ_TABLE`: The BigQuery table name for flight data.\n",
        "- `FLIGHT_RECORD_LIMIT`: A pipeline setting to limit records from the API.\n",
        "\n",
        "Finally, it sets the `gcloud` project configuration to the specified `PROJECT_ID`.\n",
        "R\"\"\"\n",
        "# --- !! CONFIGURE YOUR VARIABLES !! ---\n",
        "\n",
        "PROJECT_ID = \"mgmt467-471119\"\n",
        "GCP_REGION = \"us-central1\"  # Or the region you are using\n",
        "\n",
        "# --- GCS Bucket (Source & Target) ---\n",
        "GCS_BUCKET_NAME = \"mgmt467-471119opensky\"\n",
        "GCS_FOLDER_PATH = \"opensky-data\" # The folder you set in your scheduler\n",
        "\n",
        "# --- BigQuery Table (Target) ---\n",
        "BQ_DATASET = \"training_dataset\" # The dataset you created\n",
        "BQ_TABLE = \"flight_data\"        # The table for flight data\n",
        "\n",
        "# --- Pipeline Settings ---\n",
        "FLIGHT_RECORD_LIMIT = 500 # How many records to pull from the API\n",
        "\n",
        "# -------------------------------------\n",
        "\n",
        "# Set the project for all gcloud commands\n",
        "!gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859fc125"
      },
      "source": [
        "## Cell 3: Define the `OpenSkyApi` class and helper functions for data parsing and formatting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe4d60cb",
        "outputId": "75b5ac7c-2cf2-4a66-f70c-e8dd870fd142"
      },
      "source": [
        "R\"\"\"\n",
        "This cell defines the `OpenSkyApi` class, along with `StateVector` and `OpenSkyStates` helper classes, for interacting with the OpenSky Network API.\n",
        "\n",
        "It includes utility functions to:\n",
        "- Fetch real-time flight data from the OpenSky API.\n",
        "- Handle API rate limiting.\n",
        "- Parse and convert raw API data into a structured format suitable for storage and analysis.\n",
        "\n",
        "Additionally, it defines helper functions (`_convertTimestamp`, `_convert`, `_convertRow`) for data type conversion and formatting flight records.\n",
        "R\"\"\"\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import calendar\n",
        "import time\n",
        "import pprint\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# ==============================================================================\n",
        "# OpenSky API Library Code\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('opensky_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class StateVector(object):\n",
        "    keys = [\"icao24\", \"callsign\", \"origin_country\", \"time_position\",\n",
        "            \"last_contact\", \"longitude\", \"latitude\", \"baro_altitude\", \"on_ground\",\n",
        "            \"velocity\", \"heading\", \"vertical_rate\", \"sensors\",\n",
        "            \"geo_altitude\", \"squawk\", \"spi\", \"position_source\"]\n",
        "    def __init__(self, arr):\n",
        "        self.__dict__ = dict(zip(StateVector.keys, arr))\n",
        "\n",
        "class OpenSkyStates(object):\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        if self.states is not None:\n",
        "            self.states = [StateVector(a) for a in self.states]\n",
        "        else:\n",
        "            self.states = []\n",
        "\n",
        "class OpenSkyApi(object):\n",
        "    def __init__(self, username=None, password=None):\n",
        "        self._auth = (username, password) if username else ()\n",
        "        self._api_url = \"https://opensky-network.org/api\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\",\n",
        "                         auth=self._auth, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            return r.json()\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        return None\n",
        "\n",
        "    def _check_rate_limit(self, time_diff_noauth, time_diff_auth, func):\n",
        "        if len(self._auth) < 2:\n",
        "            return abs(time.time() - self._last_requests[func]) >= time_diff_noauth\n",
        "        return abs(time.time() - self._last_requests[func]) >= time_diff_auth\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_lat(lat):\n",
        "        if not -90 <= lat <= 90:\n",
        "            raise ValueError(f\"Invalid latitude {lat}!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_lon(lon):\n",
        "        if not -180 <= lon <= 180:\n",
        "            raise ValueError(f\"Invalid longitude {lon}!\")\n",
        "\n",
        "    def get_states(self, time_secs=0, icao24=None, serials=None, bbox=()):\n",
        "        if not self._check_rate_limit(10, 5, self.get_states):\n",
        "            logger.debug(\"Blocking request due to rate limit\")\n",
        "            return None\n",
        "        t = calendar.timegm(time_secs.timetuple()) if isinstance(time_secs, datetime.datetime) else int(time_secs)\n",
        "        params = {\"time\": t, \"icao24\": icao24}\n",
        "        if len(bbox) == 4:\n",
        "            self._check_lat(bbox[0]); self._check_lat(bbox[1]); self._check_lon(bbox[2]); self._check_lon(bbox[3])\n",
        "            params.update({\"lamin\": bbox[0], \"lamax\": bbox[1], \"lomin\": bbox[2], \"lomax\": bbox[3]})\n",
        "        states_json = self._get_json(\"/states/all\", self.get_states, params=params)\n",
        "        return OpenSkyStates(states_json) if states_json else None\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp):\n",
        "    if timestamp is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def _convert(data,dataType):\n",
        "    if data is not None:\n",
        "        if dataType==str: return data.strip()\n",
        "        try: return dataType(data)\n",
        "        except: return None\n",
        "    return None\n",
        "\n",
        "def _convertRow(flightState, queryTime):\n",
        "    row = {\n",
        "        'icao24': _convert(flightState.icao24,str),\n",
        "        'callsign': _convert(flightState.callsign,str),\n",
        "        'origin': _convert(flightState.origin_country,str),\n",
        "        'time':_convert( flightState.time_position,int),\n",
        "        'contact':_convert( flightState.last_contact,int),\n",
        "        'longitude':_convert( flightState.longitude,float),\n",
        "        'latitude':_convert( flightState.latitude,float),\n",
        "        'altitude':_convert( flightState.geo_altitude,float),\n",
        "        'on_ground':_convert( flightState.on_ground,bool),\n",
        "        'velocity':_convert( flightState.velocity,float),\n",
        "        'heading':_convert( flightState.heading,float),\n",
        "        'vertical_rate':_convert( flightState.vertical_rate,float),\n",
        "        'sensors':_convert( flightState.sensors,str),\n",
        "        'baro_altitude':_convert( flightState.baro_altitude,float),\n",
        "        'squawk':_convert( flightState.squawk,int),\n",
        "        'spi':_convert( flightState.spi,bool),\n",
        "        'position_source':_convert( flightState.position_source,int)\n",
        "    }\n",
        "    time_bq = _convertTimestamp(flightState.time_position)\n",
        "    if time_bq: row['time_bq'] = time_bq\n",
        "    contact_bq = _convertTimestamp(flightState.last_contact)\n",
        "    if contact_bq: row['contact_bq'] = contact_bq\n",
        "    query_time_bq = _convertTimestamp(queryTime)\n",
        "    if query_time_bq: row['query_time_bq'] = query_time_bq\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "print(\"✅ Helper functions and OpenSky API class defined.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Helper functions and OpenSky API class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffba020"
      },
      "source": [
        "## Cell 4: Define the `OpenSkyApi` class and helper functions, initialize GCP clients, define the BigQuery schema, and implement the data pipeline logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "323deb8f",
        "outputId": "3b4a6c7a-0997-4ba2-86e5-22c7a83e8a26"
      },
      "source": [
        "R\"\"\"\n",
        "This cell defines the `OpenSkyApi` class for interacting with the OpenSky Network API,\n",
        "along with several helper functions for data processing and a complete data pipeline.\n",
        "\n",
        "Key functionalities include:\n",
        "- **OpenSky API Integration**: Classes (`StateVector`, `OpenSkyStates`, `OpenSkyApi`)\n",
        "  to fetch real-time flight data and handle API rate limits.\n",
        "- **Data Parsing & Conversion**: Helper functions (`_convertTimestamp`, `_convert`, `_convertRow`)\n",
        "  to clean and format raw API data into a structured dictionary for storage.\n",
        "- **GCP Client Initialization**: Initializes `google.cloud.storage.Client` and\n",
        "  `google.cloud.bigquery.Client` for interacting with Google Cloud Storage and BigQuery.\n",
        "- **BigQuery Schema Definition**: Defines the explicit schema (`BQ_SCHEMA`) for the\n",
        "  BigQuery `flight_data` table.\n",
        "- **GCS to BigQuery Load Function**: `load_gcs_to_bigquery` handles loading JSONL files\n",
        "  from GCS into a specified BigQuery table.\n",
        "- **Full Pipeline Logic**: `run_full_pipeline_without_bq_load` orchestrates the process\n",
        "  of fetching data from OpenSky API, processing it, and uploading it to a GCS bucket.\n",
        "\n",
        "This cell effectively sets up all necessary components for data ingestion into GCP.\n",
        "R\"\"\"\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import calendar\n",
        "import time\n",
        "import pprint\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# ==============================================================================\n",
        "# OpenSky API Library Code\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('opensky_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class StateVector(object):\n",
        "    keys = [\"icao24\", \"callsign\", \"origin_country\", \"time_position\",\n",
        "            \"last_contact\", \"longitude\", \"latitude\", \"baro_altitude\", \"on_ground\",\n",
        "            \"velocity\", \"heading\", \"vertical_rate\", \"sensors\",\n",
        "            \"geo_altitude\", \"squawk\", \"spi\", \"position_source\"]\n",
        "    def __init__(self, arr):\n",
        "        self.__dict__ = dict(zip(StateVector.keys, arr))\n",
        "\n",
        "class OpenSkyStates(object):\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        if self.states is not None:\n",
        "            self.states = [StateVector(a) for a in self.states]\n",
        "        else:\n",
        "            self.states = []\n",
        "\n",
        "class OpenSkyApi(object):\n",
        "    def __init__(self, username=None, password=None):\n",
        "        self._auth = (username, password) if username else ()\n",
        "        self._api_url = \"https://opensky-network.org/api\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\",\n",
        "                         auth=self._auth, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            return r.json()\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        return None\n",
        "\n",
        "    def _check_rate_limit(self, time_diff_noauth, time_diff_auth, func):\n",
        "        if len(self._auth) < 2:\n",
        "            return abs(time.time() - self._last_requests[func]) >= time_diff_noauth\n",
        "        return abs(time.time() - self._last_requests[func]) >= time_diff_auth\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_lat(lat):\n",
        "        if not -90 <= lat <= 90:\n",
        "            raise ValueError(f\"Invalid latitude {lat}!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_lon(lon):\n",
        "        if not -180 <= lon <= 180:\n",
        "            raise ValueError(f\"Invalid longitude {lon}!\")\n",
        "\n",
        "    def get_states(self, time_secs=0, icao24=None, serials=None, bbox=()):\n",
        "        if not self._check_rate_limit(10, 5, self.get_states):\n",
        "            logger.debug(\"Blocking request due to rate limit\")\n",
        "            return None\n",
        "        t = calendar.timegm(time_secs.timetuple()) if isinstance(time_secs, datetime.datetime) else int(time_secs)\n",
        "        params = {\"time\": t, \"icao24\": icao24}\n",
        "        if len(bbox) == 4:\n",
        "            self._check_lat(bbox[0]); self._check_lat(bbox[1]); self._check_lon(bbox[2]); self._check_lon(bbox[3])\n",
        "            params.update({\"lamin\": bbox[0], \"lamax\": bbox[1], \"lomin\": bbox[2], \"lomax\": bbox[3]})\n",
        "        states_json = self._get_json(\"/states/all\", self.get_states, params=params)\n",
        "        return OpenSkyStates(states_json) if states_json else None\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp):\n",
        "    if timestamp is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def _convert(data,dataType):\n",
        "    if data is not None:\n",
        "        if dataType==str: return data.strip()\n",
        "        try: return dataType(data)\n",
        "        except: return None\n",
        "    return None\n",
        "\n",
        "def _convertRow(flightState, queryTime):\n",
        "    row = {\n",
        "        'icao24': _convert(flightState.icao24,str),\n",
        "        'callsign': _convert(flightState.callsign,str),\n",
        "        'origin': _convert(flightState.origin_country,str),\n",
        "        'time':_convert( flightState.time_position,int),\n",
        "        'contact':_convert( flightState.last_contact,int),\n",
        "        'longitude':_convert( flightState.longitude,float),\n",
        "        'latitude':_convert( flightState.latitude,float),\n",
        "        'altitude':_convert( flightState.geo_altitude,float),\n",
        "        'on_ground':_convert( flightState.on_ground,bool),\n",
        "        'velocity':_convert( flightState.velocity,float),\n",
        "        'heading':_convert( flightState.heading,float),\n",
        "        'vertical_rate':_convert( flightState.vertical_rate,float),\n",
        "        'sensors':_convert( flightState.sensors,str),\n",
        "        'baro_altitude':_convert( flightState.baro_altitude,float),\n",
        "        'squawk':_convert( flightState.squawk,int),\n",
        "        'spi':_convert( flightState.spi,bool),\n",
        "        'position_source':_convert( flightState.position_source,int)\n",
        "    }\n",
        "    time_bq = _convertTimestamp(flightState.time_position)\n",
        "    if time_bq: row['time_bq'] = time_bq\n",
        "    contact_bq = _convertTimestamp(flightState.last_contact)\n",
        "    if contact_bq: row['contact_bq'] = contact_bq\n",
        "    query_time_bq = _convertTimestamp(queryTime)\n",
        "    if query_time_bq: row['query_time_bq'] = query_time_bq\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "# Initialize GCP clients\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# This is our robust, explicit schema\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"icao24\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"callsign\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"origin\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"time\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"contact\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"longitude\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"latitude\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"altitude\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"on_ground\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"velocity\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"heading\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"vertical_rate\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"sensors\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"baro_altitude\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"squawk\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"spi\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"position_source\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"time_bq\", \"TIMESTAMP\"),\n",
        "    bigquery.SchemaField(\"contact_bq\", \"TIMESTAMP\"),\n",
        "    bigquery.SchemaField(\"query_time_bq\", \"TIMESTAMP\"),\n",
        "]\n",
        "\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"✅ GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline():\n",
        "    \"\"\"Executes the API -> GCS pipeline and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from OpenSky API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching up to {FLIGHT_RECORD_LIMIT} records from OpenSky API...\")\n",
        "        api = OpenSkyApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "        flightStates = api.get_states()\n",
        "\n",
        "        records = []\n",
        "        if flightStates and flightStates.states:\n",
        "            for state in flightStates.states:\n",
        "                if len(records) >= FLIGHT_RECORD_LIMIT:\n",
        "                    break\n",
        "                records.append(_convertRow(state, queryTime))\n",
        "\n",
        "        if not records:\n",
        "            print(\"No flight data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} records.\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"flight_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/colab_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"✅ API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Main pipeline functions defined.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Main pipeline functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c8be80"
      },
      "source": [
        "## Cell 5: Execute the complete data pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683da7fa",
        "outputId": "97fe55b0-7fd5-4010-f151-4d6ee0ae7a1b"
      },
      "source": [
        "R\"\"\"\n",
        "This cell is intended to execute the full data pipeline, which typically involves:\n",
        "1. Fetching flight data from the OpenSky API.\n",
        "2. Uploading the fetched data to Google Cloud Storage (GCS).\n",
        "3. Loading the data from GCS into a BigQuery table.\n",
        "\n",
        "Note: The `run_full_pipeline()` function is assumed to be defined elsewhere in the notebook,\n",
        "encapsulating these steps.\n",
        "R\"\"\"\n",
        "run_full_pipeline()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Fetching up to 500 records from OpenSky API...\n",
            "  > Fetched 500 records.\n",
            "\n",
            "Step 2: Uploading data to GCS...\n",
            "  > Source: flight_data.jsonl\n",
            "  > Destination: gs://mgmt467-471119opensky/opensky-data/colab_batch_1763941396.jsonl\n",
            "\n",
            "❌ ERROR in pipeline: 404 POST https://storage.googleapis.com/upload/storage/v1/b/mgmt467-471119opensky/o?uploadType=multipart: {\n",
            "  \"error\": {\n",
            "    \"code\": 404,\n",
            "    \"message\": \"The specified bucket does not exist.\",\n",
            "    \"errors\": [\n",
            "      {\n",
            "        \"message\": \"The specified bucket does not exist.\",\n",
            "        \"domain\": \"global\",\n",
            "        \"reason\": \"notFound\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ": ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ff6f7b"
      },
      "source": [
        "## Cell 6: Orchestrate the full data pipeline from API to GCS to BigQuery.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "426ba363",
        "outputId": "70d832c0-ca84-4540-95c2-b638f422e23e"
      },
      "source": [
        "R\"\"\"\n",
        "This cell orchestrates the entire data pipeline, executing the following steps:\n",
        "1.  **Run API to GCS Pipeline**: It calls `run_full_pipeline_without_bq_load()` to fetch\n",
        "    flight data from the OpenSky API and upload it as a JSONL file to a Google Cloud Storage bucket.\n",
        "2.  **Load GCS to BigQuery**: If the GCS upload is successful, it then calls\n",
        "    `load_gcs_to_bigquery()` to load the data from the GCS URI into the specified BigQuery table.\n",
        "\n",
        "This ensures a complete data ingestion workflow from an external API to a data warehouse.\n",
        "R\"\"\"\n",
        "print(\"--- Running Full Data Pipeline (API -> GCS -> BigQuery) ---\")\n",
        "\n",
        "# 1. Execute API -> GCS pipeline\n",
        "gcs_uri_for_bq_load = run_full_pipeline_without_bq_load()\n",
        "\n",
        "if gcs_uri_for_bq_load:\n",
        "    # 2. Load data from GCS to BigQuery\n",
        "    load_gcs_to_bigquery(\n",
        "        gcs_uri_for_bq_load,\n",
        "        PROJECT_ID,\n",
        "        BQ_DATASET,\n",
        "        BQ_TABLE,\n",
        "        BQ_SCHEMA,\n",
        "        bq_client\n",
        "    )\n",
        "    print(\"✅ Pipeline Finished Successfully.\")\n",
        "else:\n",
        "    print(\"❌ Pipeline aborted: No data fetched or GCS upload failed.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Full Data Pipeline (API -> GCS -> BigQuery) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_full_pipeline_without_bq_load' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3334091290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 1. Execute API -> GCS pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgcs_uri_for_bq_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_pipeline_without_bq_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgcs_uri_for_bq_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_full_pipeline_without_bq_load' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a75f8d"
      },
      "source": [
        "## Cell 7: Create a BQML regression model to predict flight velocity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48c9295f",
        "outputId": "a907299f-2607-47cc-8f11-524897ee534a"
      },
      "source": [
        "R\"\"\"\n",
        "This cell creates a BigQuery ML (BQML) regression model.\n",
        "\n",
        "Purpose:\n",
        "- **Model Type**: Initializes a `LINEAR_REG` model.\n",
        "- **Objective**: To predict `velocity` (the label) of flights.\n",
        "- **Features**: Uses `altitude`, `vertical_rate`, and `heading` as input features.\n",
        "- **Data Filtering**: Excludes `on_ground` flights (`on_ground = false`) to focus on airborne velocity prediction.\n",
        "- **Output**: Stores the trained model in BigQuery at the path defined by `REGRESSION_MODEL_NAME`.\n",
        "\n",
        "The cell also prints basic training statistics upon successful model creation.\n",
        "R\"\"\"\n",
        "print(\"--- Cell 6: Creating BQML Prediction (Regression) Model ---\")\n",
        "\n",
        "# Define the model name\n",
        "REGRESSION_MODEL_NAME = \"flight_velocity_predictor\"\n",
        "model_path = f\"{PROJECT_ID}.{BQ_DATASET}.{REGRESSION_MODEL_NAME}\"\n",
        "\n",
        "# This SQL query creates a linear regression model.\n",
        "# We are trying to PREDICT the 'velocity' (the LABEL)\n",
        "# using 'altitude', 'vertical_rate', and 'heading' (the FEATURES).\n",
        "# We also filter out any on-ground flights, as we only want to predict airborne velocity.\n",
        "\n",
        "CREATE_PREDICTION_MODEL_QUERY = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{model_path}`\n",
        "OPTIONS(\n",
        "    model_type='LINEAR_REG',\n",
        "    input_label_cols=['velocity']\n",
        ") AS\n",
        "SELECT\n",
        "    velocity,\n",
        "    altitude,\n",
        "    vertical_rate,\n",
        "    heading\n",
        "FROM\n",
        "    `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "WHERE\n",
        "    velocity IS NOT NULL\n",
        "    AND altitude IS NOT NULL\n",
        "    AND vertical_rate IS NOT NULL\n",
        "    AND heading IS NOT NULL\n",
        "    AND on_ground = false\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating regression model at: {model_path}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Run the BQML query\n",
        "try:\n",
        "    regression_job = bq_client.query(CREATE_PREDICTION_MODEL_QUERY)\n",
        "    regression_job.result()  # Wait for the model training job to complete\n",
        "\n",
        "    print(f\"✅ Successfully created prediction model: {REGRESSION_MODEL_NAME}\")\n",
        "\n",
        "    # Optional: Get basic training stats\n",
        "    stats_query = f\"SELECT * FROM ML.TRAINING_INFO(MODEL `{model_path}`)\"\n",
        "    stats_job = bq_client.query(stats_query)\n",
        "    for row in stats_job.result():\n",
        "        print(f\"  > Iteration {row['iteration']}: Loss = {row['loss']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error creating regression model: {e}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Cell 6: Creating BQML Prediction (Regression) Model ---\n",
            "Creating regression model at: mgmt467-471119.training_dataset.flight_velocity_predictor\n",
            "This may take a few minutes...\n",
            "❌ Error creating regression model: 404 Not found: Table mgmt467-471119:training_dataset.flight_data was not found in location us-central1; reason: notFound, message: Not found: Table mgmt467-471119:training_dataset.flight_data was not found in location us-central1\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 251a2bcb-fff0-4924-97a1-18a840dbe944\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09306c15"
      },
      "source": [
        "## Cell 8: Analyze `on_ground` label diversity in BigQuery\n",
        "\n",
        "Investigate the distribution of the `on_ground` column in the `flight_data` BigQuery table to understand why the classification model failed due to insufficient label diversity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2371344",
        "outputId": "bb2181fe-61f3-4fd6-aa5e-eb105afcb0f7"
      },
      "source": [
        "print(\"--- Analyzing 'on_ground' label diversity ---\")\n",
        "\n",
        "# Construct the SQL query to count distinct 'on_ground' values\n",
        "ANALYZE_ON_GROUND_QUERY = f\"\"\"\n",
        "SELECT\n",
        "    on_ground,\n",
        "    COUNT(*)\n",
        "FROM\n",
        "    `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "GROUP BY\n",
        "    on_ground\n",
        "ORDER BY\n",
        "    on_ground DESC\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Executing query to analyze 'on_ground' distribution:\\n{ANALYZE_ON_GROUND_QUERY}\")\n",
        "\n",
        "# Execute the query\n",
        "try:\n",
        "    query_job = bq_client.query(ANALYZE_ON_GROUND_QUERY)\n",
        "    results = query_job.result()  # Wait for the query to complete\n",
        "\n",
        "    print(\"\\nResults for 'on_ground' distribution:\")\n",
        "    found_true = False\n",
        "    found_false = False\n",
        "    for row in results:\n",
        "        print(f\"  on_ground: {row['on_ground']}, Count: {row['f0_']}\")\n",
        "        if row['on_ground'] is True:\n",
        "            found_true = True\n",
        "        if row['on_ground'] is False:\n",
        "            found_false = True\n",
        "\n",
        "    if found_true and found_false:\n",
        "        print(\"✅ 'on_ground' column contains both TRUE and FALSE values. Label diversity is present.\")\n",
        "    elif found_true:\n",
        "        print(\"❌ 'on_ground' column contains only TRUE values. Insufficient label diversity for classification.\")\n",
        "    elif found_false:\n",
        "        print(\"❌ 'on_ground' column contains only FALSE values. Insufficient label diversity for classification.\")\n",
        "    else:\n",
        "        print(\"⚠️ No data found for 'on_ground' column.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error analyzing 'on_ground' diversity: {e}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Analyzing 'on_ground' label diversity ---\n",
            "Executing query to analyze 'on_ground' distribution:\n",
            "\n",
            "SELECT\n",
            "    on_ground,\n",
            "    COUNT(*)\n",
            "FROM\n",
            "    `mgmt467-471119.training_dataset.flight_data`\n",
            "GROUP BY\n",
            "    on_ground\n",
            "ORDER BY\n",
            "    on_ground DESC\n",
            "\n",
            "❌ Error analyzing 'on_ground' diversity: 404 Not found: Table mgmt467-471119:training_dataset.flight_data was not found in location us-central1; reason: notFound, message: Not found: Table mgmt467-471119:training_dataset.flight_data was not found in location us-central1\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 209beb82-2bfc-4e65-a359-11e5718cbf9d\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc27f768"
      },
      "source": [
        "## Cell 9: Create a BQML classification model to predict whether a flight is on the ground.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54196fb1",
        "outputId": "a2f30972-95ed-4626-f55b-9b368aae3e71"
      },
      "source": [
        "R\"\"\"\n",
        "This cell creates a BigQuery ML (BQML) classification model.\n",
        "\n",
        "Purpose:\n",
        "- **Model Type**: Initializes a `LOGISTIC_REG` model.\n",
        "- **Objective**: To classify whether a flight is `on_ground` (the label).\n",
        "- **Features**: Uses `altitude` and `velocity` as input features.\n",
        "- **Data Split Method**: Uses `NO_SPLIT` to ensure the model trains on all available data,\n",
        "  which is crucial for the classification task to see both 'true' and 'false' labels.\n",
        "- **Output**: Stores the trained model in BigQuery at the path defined by `CLASSIFICATION_MODEL_NAME`.\n",
        "\n",
        "The cell also prints basic training statistics upon successful model creation.\n",
        "R\"\"\"\n",
        "\n",
        "\n",
        "print(\"\\n--- Creating BQML Classification Model (with NULL handling) ---\")\n",
        "\n",
        "# Define the model name\n",
        "CLASSIFICATION_MODEL_NAME = \"flight_on_ground_classifier\"\n",
        "model_path = f\"{PROJECT_ID}.{BQ_DATASET}.{CLASSIFICATION_MODEL_NAME}\"\n",
        "\n",
        "# Modified query to handle NULLs for altitude and velocity when on_ground is TRUE\n",
        "# We'll COALESCE NULL altitude/velocity to 0 for on_ground=TRUE records\n",
        "# and still filter out other NULLs for accuracy for on_ground=FALSE records.\n",
        "CREATE_CLASSIFICATION_MODEL_QUERY = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{model_path}`\n",
        "OPTIONS(\n",
        "    model_type='LOGISTIC_REG',\n",
        "    input_label_cols=['on_ground'],\n",
        "    data_split_method='NO_SPLIT'\n",
        ") AS\n",
        "SELECT\n",
        "    CAST(on_ground AS INT64) AS on_ground,\n",
        "    COALESCE(altitude, 0) AS altitude,\n",
        "    COALESCE(velocity, 0) AS velocity\n",
        "FROM\n",
        "    `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "WHERE\n",
        "    on_ground IS NOT NULL\n",
        "    AND ( (on_ground IS TRUE AND (altitude IS NOT NULL OR velocity IS NOT NULL)) OR on_ground IS FALSE )\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating classification model at: {model_path}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "try:\n",
        "    classification_job = bq_client.query(CREATE_CLASSIFICATION_MODEL_QUERY)\n",
        "    classification_job.result()  # Wait for the model training job to complete\n",
        "\n",
        "    print(f\"✅ Successfully created classification model: {CLASSIFICATION_MODEL_NAME}\")\n",
        "\n",
        "    # Optional: Get basic training stats\n",
        "    stats_query = f\"SELECT * FROM ML.TRAINING_INFO(MODEL `{model_path}`)\"\n",
        "    stats_job = bq_client.query(stats_query)\n",
        "    for row in stats_job.result():\n",
        "        print(f\"  > Iteration {row['iteration']}: Loss = {row['loss']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error creating classification model: {e}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Creating BQML Classification Model (with NULL handling) ---\n",
            "Creating classification model at: mgmt467-471119.training_dataset.flight_on_ground_classifier\n",
            "This may take a few minutes...\n",
            "❌ Error creating classification model: 404 Not found: Table mgmt467-471119:training_dataset.flight_data was not found in location us-central1; reason: notFound, message: Not found: Table mgmt467-471119:training_dataset.flight_data was not found in location us-central1\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 01abbc80-b050-49b2-a656-11467ad98b92\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1d39797"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**Why did the BQML classification model initially fail, and how was the issue resolved?**\n",
        "The BQML classification model initially failed because the `WHERE` clause in its creation query inadvertently filtered out all records where `on_ground` was `TRUE`. This happened because all 383 `on_ground=TRUE` records also had `NULL` values for either `altitude` or `velocity`, and the original `WHERE` clause explicitly excluded records with `NULL`s in these feature columns. The issue was resolved by modifying the model creation query to use `COALESCE(altitude, 0)` and `COALESCE(velocity, 0)` to handle these `NULL` values and adjusting the `WHERE` clause to ensure `on_ground=TRUE` records were included for training.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The initial attempt to create a BigQuery ML logistic regression model (`flight_on_ground_classifier`) for predicting `on_ground` status failed with an error stating \"Classification model requires at least 2 unique labels and the label column had only 1 unique label.\"\n",
        "*   Analysis of the `flight_data` BigQuery table confirmed that the `on_ground` column contained both `TRUE` (383 records) and `FALSE` (4117 records) values, indicating sufficient label diversity in the raw dataset.\n",
        "*   A diagnostic query revealed that the `WHERE` clause used in the model creation (`on_ground IS NOT NULL AND altitude IS NOT NULL AND velocity IS NOT NULL`) was the root cause, as it filtered out all records where `on_ground` was `TRUE`.\n",
        "*   Further investigation confirmed that all 383 records with `on_ground=TRUE` also had `NULL` values for either `altitude` or `velocity`, explaining why they were excluded by the `WHERE` clause.\n",
        "*   The `flight_velocity_predictor` BQML linear regression model was successfully created.\n",
        "*   After modifying the classification model query to use `COALESCE(altitude, 0)` and `COALESCE(velocity, 0)` for `NULL` handling and adjusting the `WHERE` clause, the `flight_on_ground_classifier` BigQuery ML logistic regression model was successfully created and trained.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   It is crucial to verify data conditions after applying filtering clauses, especially in `WHERE` statements for model training, as filtering can inadvertently remove necessary label diversity or critical data points.\n",
        "*   The `COALESCE` function proved effective in handling `NULL` values in feature columns, allowing for the inclusion of relevant data points that would otherwise be excluded, enabling successful model training.\n"
      ]
    }
  ]
}
